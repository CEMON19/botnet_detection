# -*- coding: utf-8 -*-
"""CNN Data Preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sd8yzr0q0gqMFKtkyOEzKVm3CS8G5VVT
"""

import numpy as np
import pandas as pd
import matplotlib
import seaborn as sb
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import os
import cv2
import keras
import tensorflow
from keras.models import Sequential,Input,Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
import matplotlib.pyplot as plt


from keras.models import model_from_json

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import BatchNormalization


columns=[
        'duration',
'protocol_type', 
'service' , 
'flag' ,
 'src_bytes' ,
 'dst_bytes',
'land',
'wrong_fragment',
 'urgent' ,
'hot',
'num_failed_logins',
 'logged_in',
 'num_compromised',
 'root_shell' ,
 'su_attempted' ,
 'num_root' ,
 'num_file_creations' ,
 'num_shells' ,
 'num_access_files',
'num_outbound_cmds' ,
 'is_host_login',
 'is_guest_login',
 'count',
 'srv_count' ,
 'serror_rate' ,
 'srv_serror_rate' ,
 'rerror_rate' ,
 'srv_rerror_rate' ,
 'same_srv_rate' ,
 'diff_srv_rate' ,
 'srv_diff_host_rate',
 'dst_host_count',
 'dst_host_srv_count' ,
 'dst_host_same_srv_rate' ,
 'dst_host_diff_srv_rate' ,
 'dst_host_same_src_port_rate' ,
 'dst_host_srv_diff_host_rate' ,
 'dst_host_serror_rate' ,
 'dst_host_srv_serror_rate' ,
 'dst_host_rerror_rate',
 'dst_host_srv_rerror_rate' ,
 'class'
]

len(columns)

def Standardization(X_original):
  scaler1 = StandardScaler().fit(X)
  X_original=scaler1.transform(X)
  return X_original

data=pd.read_csv("KDD Train+ Unwanted removed.txt")

data.columns=columns

print(data.head())

print(data.tail())

data['duration'].count()

data['class'].value_counts()

print(data.describe())

for i in columns:
  if type(data[i][0])==type(data['protocol_type'][0]):
    print(i+" column has "+str(data[i].nunique())+" unique features")



cat_col=['protocol_type','service','flag','class']
new_categorical_columns=data[cat_col]
new_categorical_columns.head()

new_cat_encoded=new_categorical_columns.apply(LabelEncoder().fit_transform)

print(new_cat_encoded.head())

data=data.drop(['flag','protocol_type','service'],axis=1)

data=data.drop('class',axis=1)

data=data.join(new_cat_encoded)

data = data.reindex(columns, axis=1)
print(data.head())

X = data.drop('class',1)
Y = data['class']

Y_original=Y.values

print(Y_original)

"""Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.

Example: If an algorithm is not using the feature scaling method then it can consider the value 3000 meters to be greater than 5 km but thatâ€™s actually not true and in this case, the algorithm will give wrong predictions. So, we use Feature Scaling to bring all values to the same magnitudes and thus, tackle this issue.

Techniques to perform Feature Scaling
Consider the two most important ones:

Min-Max Normalization: This technique re-scales a feature or observation value with distribution value between 0 and 1.
X_{\text {new }}=\frac{X_{i}-\min (X)}{\max (x)-\min (X)}

Standardization: It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.

X_{\text {new }}=\frac{X_{i}-X_{\text {mean }}}{\text { Standard Deviation }}

PLOTTING TO SEE HOW MANY NORMAL AND ANAMOLY TRAFFIC EXISTS, NORMAL 1,ANAMOLY 0
"""

print(X)

X = data.drop(['land','urgent','num_failed_logins','root_shell','su_attempted','num_root' ,'num_shells',
'num_access_files','num_outbound_cmds','is_host_login','serror_rate','srv_rerror_rate'],1)

print(len(X.columns))

X_original=Standardization(X)

"""Y_original is the true Y to be predicted"""

import pickle
filename='/content/kmeans_cluster.pkl'
infile = open(filename,'rb')
kmeans = pickle.load(infile)
Y_train_new=kmeans.predict(X_original)



print(len(X_original[0]))

Y_train_new = to_categorical(Y_train_new)

print(Y_train_new)

"""Converting Data into images of 5X6"""

array=np.reshape(X_original[5]*255, (5,6))
plt.imshow(array,cmap='gray')

def convertToImage(X_original):
  x=list()
  count=0
  for i in X_original:
    array = np.reshape(i, (5,6))
    print(count)
    count+=1
    img=np.reshape(array,(array.shape[0],array.shape[1],1))
    x.append(img)
  X=np.array(x)
  return X

X_images=convertToImage(X_original)

# building a linear stack of layers with the sequential model
model = Sequential()
# convolutional layer
model.add(Input(shape=(5,6,1)))
model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(1,1)))
# flatten output of conv
model.add(Flatten())
# hidden layer
model.add(Dense(100, activation='relu'))
# output layer
model.add(Dense(4, activation='softmax'))
model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])
model.summary()

m_train=model.fit(X_images,Y_train_new,batch_size=64,verbose=1,epochs=40)

accuracy=m_train.history['accuracy']
#val_accuracy=m_train.history['val_accuracy']
loss=m_train.history['loss']
#val_loss=m_train.history['val_loss']
epochs=range(len(loss))
plt.figure()
#plt.plot(epochs,val_loss,'b',label='Validation loss')
plt.plot(epochs,accuracy,'--',label='Training accuracy')
#plt.plot(epochs,val_accuracy,'b',label='Validation accuracy')
plt.title("Accuracy")
plt.legend()
plt.figure()
plt.plot(epochs,loss,'--',label='Training loss')
#plt.plot(epochs,val_loss,'b',label='Validation loss')
plt.title("Loss")
plt.legend()
plt.show()

CNN_json = model.to_json()
with open("/content/CNN_kmeans.json", "w") as json_file:
  json_file.write(CNN_json)
# serialize weights to HDF5
model.save_weights("CNN_kmeans.h5")
print("Saved model to disk")

